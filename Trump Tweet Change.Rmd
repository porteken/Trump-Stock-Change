---
title: "R Notebook"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{R message=F}
library(tidyverse)
library(reticulate)
library(quantmod)
library(rtweet)
library(data.table)
library(lubridate)
library(e1071)
library(DT)
library(feasts)
library(caret)
library(fable)
library(data.table)
library(tsibble)
library(mlr3measures)
library(mlr3verse)
knitr::knit_engines$set(python=reticulate::eng_python)
```

```{R}
data<-fread('train.csv')
data$created_at<-with_tz(parse_date_time(data$created_at,'%m/%d/%Y H:M'),tzone='US/Central')
dim(data)
data<-data %>% mutate(date=if_else(am(created_at) | wday(created_at) %in% c(7,1),date(created_at),date(created_at+days(1))),is_retweet=as.factor(if_else(is_retweet=='TRUE',1,0))) %>% select(-one_of('created_at'))
dim(data)
```

```{R warning=F}
data$text<-gsub('http\\S+\\s*','',data$text)
data$text<-gsub("[^0-9A-Za-z///' ]",'',data$text,ignore.case = T)
data<-data %>% filter(text!='')
datatable(head(data))
```

```{python}
import numpy as np
import pandas as pd
import torch
from numpy.testing import assert_almost_equal
import re
import spacy
import string
from spacy.tokens import DocBin
import os
from pathlib import Path
from joblib import Parallel, delayed
from functools import partial
import thinc.extra.datasets
import plac
import spacy
from spacy.util import minibatch
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
```

```{Python}
from spacy_transformers import TransformersLanguage, TransformersWordPiecer, TransformersTok2Vec
name = "bert-base-uncased"
nlp = TransformersLanguage(trf_name=name, meta={"lang": "en"})
nlp.add_pipe(nlp.create_pipe("sentencizer"))
nlp.add_pipe(TransformersWordPiecer.from_pretrained(nlp.vocab, name))
nlp.add_pipe(TransformersTok2Vec.from_pretrained(nlp.vocab, name))
print(nlp.pipe_names) 
```

```{python}
def transform_texts(nlp,batch_id,texts):
  df=df.append([doc.vector_norm for doc in texts])
  df2=df2.append([doc.text for doc in texts])
  
trains=r.data
nlp = spacy.load("en_trf_bertbaseuncased_lg")
texts= list(nlp.pipe(trains['text']))
partitions = minibatch(texts, size=1000)
df = pd.Series()
df2= pd.Series()
executor = Parallel(n_jobs=-1, prefer="threading")
do = delayed(partial(transform_texts, nlp))
tasks = (do(i, batch) for i, batch in enumerate(partitions))
executor(tasks)
trains['score']=df
trains['text']=df2
trains.head()
```

```{python warning=F}
trains.shape
trains['date']=pd.to_datetime(trains['date'])
print(trains.head())
```

```{R message=F,tidy=T}
data<-py$trains
data$date<-date(data$date)
prices<-fread('SPY 2.csv',col.names = c('date','change'))
prices$date<-mdy(prices$date)
data<-full_join(data,prices,by='date')
data<-data %>% mutate(weekend=if_else(wday(date) %in% c(6,7),1,0),change=as.factor(change),score=lead(score,1))  %>%  fill(change,.direction='up') %>% filter(score>0) %>% select(-one_of('date','text')) 
data$is_retweet<-as.numeric(data$is_retweet)-1
train<-data[1:round(nrow(data)*.95-1,0),]
test<-data[round(nrow(data)*.95-1,0):nrow(data),]
data.table(data)
```

```{R warning=F}
lgr::get_logger("mlr3")$set_threshold("warn")
task<-TaskClassif$new('task',train,'change',positive='1')
benchmarks<-benchmark_grid(task,list(lrn('classif.xgboost'),lrn('classif.ranger'),lrn('classif.svm'),lrn('classif.log_reg'),lrn('classif.naive_bayes')),rsmp('cv',folds=10))
future::plan('multicore')
bench<-benchmark(benchmarks)
```

```{R}
bench$aggregate() %>% arrange(classif.ce)
```

```{R}
tuner<-tnr('random_search',batch_size=36)
searchspace<-ParamSet$new(list(ParamInt$new('num.trees',lower = 100,upper=1200),ParamInt$new('max.depth',lower=5,upper=30)))
tuner<-AutoTuner$new(lrn('classif.ranger'),rsmp('cv',folds=10),msr('classif.ce'),searchspace,term('evals',n_evals=36),tuner)
model<-tuner$train(task)
model$tuning_result
```

```{R}
testing<-TaskClassif$new('test',test,'change',positive='1')
result<-model$predict(testing)
confusion_matrix(result$truth,result$response,positive='1')
```